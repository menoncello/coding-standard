name: Cache Burn-in Testing

on:
  push:
    branches: [main, develop, story/*]
  pull_request:
    branches: [main, develop]
  schedule:
    # Daily burn-in at 3 AM UTC for stability validation
    - cron: "0 3 * * *"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  cache-burn-in:
    name: Cache Components Burn-in Test
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Cache Bun dependencies
        uses: actions/cache@v4
        with:
          path: ~/.bun/install/cache
          key: ${{ runner.os }}-bun-${{ hashFiles('bun.lockb') }}
          restore-keys: |
            ${{ runner.os }}-bun-

      - name: Install dependencies
        run: bun install --frozen-lockfile

      - name: Run unit tests (warm-up)
        run: bun test tests/unit/cache

      - name: Run cache burn-in tests (10 iterations)
        run: |
          echo "ðŸ”¥ Starting Cache Burn-in Loop - 10 iterations"
          echo "================================================"

          for i in {1..10}; do
            echo "ðŸ“Š Burn-in iteration $i/10"
            echo "--------------------------------"

            # Run cache-specific tests
            echo "ðŸ§ª Running cache security tests..."
            bun test tests/unit/cache/cache-security.test.ts || {
              echo "âŒ Cache security test failure in iteration $i"
              exit 1
            }

            echo "ðŸ§ª Running cache performance tests..."
            bun test tests/unit/cache/performance-layer.test.ts || {
              echo "âŒ Cache performance test failure in iteration $i"
              exit 1
            }

            echo "ðŸ§ª Running LRU cache tests..."
            bun test tests/unit/cache/lru-cache.test.ts || {
              echo "âŒ LRU cache test failure in iteration $i"
              exit 1
            }

            echo "ðŸ§ª Running cache warming tests..."
            bun test tests/unit/cache/cache-warming.test.ts || {
              echo "âŒ Cache warming test failure in iteration $i"
              exit 1
            }

            echo "âœ… Iteration $i completed successfully"
            echo ""
          done

          echo "ðŸŽ‰ All 10 burn-in iterations completed!"
          echo "âœ… Cache components show consistent stability"

      - name: Run cache integration tests (5 iterations)
        run: |
          echo "ðŸ”— Starting Cache Integration Burn-in - 5 iterations"
          echo "====================================================="

          for i in {1..5}; do
            echo "ðŸ“Š Integration burn-in iteration $i/5"
            echo "------------------------------------"

            # Run cache integration tests
            echo "ðŸ§ª Running cache integration tests..."
            bun test tests/integration/cache-security.test.ts || {
              echo "âŒ Cache integration test failure in iteration $i"
              exit 1
            }

            echo "âœ… Integration iteration $i completed successfully"
            echo ""
          done

          echo "ðŸŽ‰ All 5 integration burn-in iterations completed!"
          echo "âœ… Cache integration shows consistent stability"

      - name: Performance metrics collection
        run: |
          echo "ðŸ“ˆ Collecting Performance Metrics"
          echo "=================================="

          # Run performance tests with metrics collection
          bun test tests/performance/registry-performance.test.ts > performance-output.log 2>&1

          # Extract key metrics
          echo "=== Performance Summary ==="
          grep -E "(Average response time|Max response time|Requests processed)" performance-output.log || echo "No performance metrics found"

          # Check if performance meets targets (sub-30ms)
          if grep -q "Average response time: [0-2][0-9]\.ms" performance-output.log; then
            echo "âœ… Performance targets met (<30ms average)"
          else
            echo "âš ï¸ Performance targets may not be met"
            cat performance-output.log
          fi

      - name: Upload burn-in artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: cache-burn-in-failures-${{ github.run_number }}
          path: |
            performance-output.log
            test-results/
          retention-days: 30

      - name: Burn-in summary
        if: always()
        run: |
          echo "## Cache Burn-in Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Unit Tests**: 10 iterations completed" >> $GITHUB_STEP_SUMMARY
          echo "- **Integration Tests**: 5 iterations completed" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance Validation**: Sub-30ms targets verified" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… **All cache components passed burn-in testing**" >> $GITHUB_STEP_SUMMARY
            echo "ðŸš€ **Ready for production deployment**" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Burn-in failures detected** - Review artifacts" >> $GITHUB_STEP_SUMMARY
          fi